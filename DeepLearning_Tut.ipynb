{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts : \n",
    "\n",
    "Why Prompts As template\n",
    "This is to replace a single/multiple vraible in a template prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing : \n",
    "\n",
    "another usecase of langchain is Output Parsing\n",
    "Without it you need to conver the outuput of llm into your structure. It sometimes is cumbercome and produces error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "React =>\n",
    "Thoughts, Actions and Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Parser ?\n",
    "Suppose u query to get some response, and the response came in a key-value format(dict), and u assgined it to a varaible, Then when using this variable, and calling the information according to the key, u will be getting error.\n",
    "\n",
    "This is bcoz. Response is purely string and u can't treat it as data structure.\n",
    "\n",
    "If need to parse ouput many times, lanchain output parser helps a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory : \n",
    "\n",
    "For recognizing the previous conversations and use it as context for next query as input.\n",
    "Lanchain provide memory in a simple way -> Conversation Buffer Memory\n",
    "can print the stored data using memory.buffer()\n",
    "# more buffer history =>  more tokens will be used\n",
    "\n",
    "# for limiting the buffer history\n",
    "# use -> Window Buffer Memory()\n",
    "\n",
    "\n",
    "# Another way of doing this is Storing \" SUMMARY \" for decreasing the tokens\n",
    "# SummaryBufferMemory()\n",
    "# all the previous conversation stored as summary in Buffer\n",
    "\n",
    "# Token Buffer Memory -> Limits according to tokens utilised \n",
    "\n",
    "\n",
    "# Types of mem\n",
    "1) Conventional \n",
    "2) Window\n",
    "3) Token\n",
    "4) Summary\n",
    "\n",
    "# Additional Memory Type\n",
    "1) Vector data Memory (stored in Vector database)\n",
    "2) Entity Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains : \n",
    "\n",
    "For carrying out a sequence of Operations(Chain of Blocks) \n",
    "ONe Output is Input to other block\n",
    "Chain helps us to run the Model over many input at once carrying a sequence of tasks.\n",
    "\n",
    "Chain or LLMChain\n",
    "LLM Chain : chain instance is simply a combination of llm and prompt\n",
    "chain = LLMChain(llm=model, prompt=query)\n",
    "\n",
    "\n",
    "Simple Sequential Chain: one input and one output\n",
    "sequential Chain -> combination of more than one chains, takes only one prompt\n",
    "For chaining, we need to set input_key and output_key of chains as parameters\n",
    "output_key of first chain will be used as input_key of the second chain --> the winding goes on \n",
    "\n",
    "Sequential Chain : Multiple Input and output\n",
    "Here Just insert all the Chains as the sequence\n",
    "Input_variables -> the first input of the key Chain\n",
    "Output_variables -> all the intermediate outputs including the final one\n",
    "\n",
    "Other Chains\n",
    "1) Router Chain\n",
    "2) MultiPromptChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding : \n",
    "Combining LLM models with local Documents\n",
    "\n",
    "LLM can't generally read very large documents of text, pdf, etc.\n",
    "But we can do it easily using embidding \n",
    "Embedding means vectorising the info in numbers and then using it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
